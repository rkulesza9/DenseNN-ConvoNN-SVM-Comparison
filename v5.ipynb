{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Convolutional Neural Network vs Neural Network vs Support Vector Machine\n## Description\n1. load the data into  memory\n1. peices and labels are in forsyth-edwards notation\n    1. this requires us to split the label into the rows of pieces \n    1. this requires us to split the rows of peices into cells\n1. images must be transformed into their individual cells\n1. use split_test_train to create a test set and train set\n1. flatten the images before using as input *(numpy_array_instance.flatten)*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\n\nfrom sklearn import svm\nfrom sklearn.model_selection import learning_curve\n\nimport numpy as np\nimport random\n\n\nfrom os import listdir\nfrom os.path import isfile, join\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom sklearn.preprocessing import LabelEncoder","execution_count":51,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Paramaters"},{"metadata":{"trusted":true},"cell_type":"code","source":"path_test = \"../input/chess-positions/test/\"\npath_train = \"../input/chess-positions/train/\"\nbatch_size = 100","execution_count":52,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing Methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"def RandomBatchLoader(directory, batch_size):\n    #get files\n    files = [directory+f for f in listdir(directory) if isfile(join(directory,f))]\n    \n    #randomize files\n    random.shuffle(files)\n    \n    #return batch_size subset of random files\n    return files[0:batch_size]\n\ndef ParseBatch(directory, batch):\n    imgs = [mpimg.imread(batch[x]) for x in range(0,len(batch))]\n    labels = [batch[x][len(directory):-5] for x in range(0,len(batch))]\n    return imgs, labels\n\ndef SplitImages(imgs):\n    imgs_split = [[img[a:a+50, b:b+50] for a in range(0,400,50) for b in range(0,400,50)] for img in imgs]\n    return imgs_split\n\ndef SplitFEN(FENS):\n    result = []\n    for x in range(0,len(FENS)):\n        result += SplitFEN_Helper(FENS[x])\n    return result\n\ndef SplitFEN_Helper(FEN):\n    FEN = FEN.replace('-','')\n    FEN = list(FEN)\n    result = []\n    for x in range(0,len(FEN)):\n        if FEN[x].isdigit():\n            for x in range(0,int(FEN[x])):\n                result.append(\"x\")\n        else:\n            result.append(FEN[x])\n    return result\n\ndef Preprocess(train_directory, test_directory, train_batch_size, test_batch_size):\n    data_train = RandomBatchLoader(train_directory, train_batch_size)\n    data_test = RandomBatchLoader(test_directory, test_batch_size)\n    \n    X_train, y_train = ParseBatch(train_directory, data_train)\n    X_test, y_test = ParseBatch(test_directory, data_test)\n    \n    X_train = SplitImages(X_train)\n    y_train = SplitFEN(y_train)\n    X_test = SplitImages(X_test)\n    y_test = SplitFEN(y_test)\n    \n    #normalize\n    X_train = np.array(X_train)\n    X_test = np.array(X_test)\n    X_train = X_train / 255\n    X_test = X_test / 255\n\n    X_train = X_train.reshape(64*train_batch_size,50,50,3)\n    X_test = X_test.reshape(64*test_batch_size,50,50,3)\n    \n    #label encode\n    label_encoder = LabelEncoder()\n    label_encoder.fit(['p','n','b','r','q','k','x', 'P', 'N', 'B', 'R', 'Q', 'K'])\n    y_train = label_encoder.transform(y_train)\n    y_test = label_encoder.transform(y_test)\n    \n    return X_train, y_train, X_test, y_test, label_encoder\n\ndef PreprocessSingle(X_file, directory, label_encoder):\n    X = [mpimg.imread(directory+X_file)]\n    y = [X_file[:-5]]\n    \n    X = SplitImages(X)\n    y = SplitFEN(y)\n    \n    X = np.array(X)\n    X = X / 255\n    y_encoded = label_encoder.transform(y)\n    \n    X = X[0]\n    \n    return X, y_encoded, y","execution_count":53,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fetch Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train, X_test, y_test, label_encoder = Preprocess(path_train, path_test, batch_size, batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_train_test_SVM(X_train, y_train, X_test, y_test):\n    #svm will look at each of 50*50*3 as a different feature\n    X_train = X_train.reshape(64*batch_size, 50*50*3)\n    X_test = X_test.reshape(64*batch_size, 50*50*3)\n    \n    clf = svm.SVC()\n    clf.fit(X_train, y_train)\n    confidence = clf.score(X_test, y_test)\n    \n    print(\"Validation Set Accuracy: \",confidence)\n    return clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = create_train_test_SVM(X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's evaluate the output of a single input"},{"metadata":{"trusted":true},"cell_type":"code","source":"file = listdir(path_test)[0]\nprint(file)\nX, y_encoded, y = PreprocessSingle(file, path_test, label_encoder)\n\nX = X.reshape(64,50*50*3)\nyp = clf.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"predicted values (encoded): \")\nprint(yp)\n\nprint(\"\\nactual values (encoded): \")\nprint(y_encoded)\n\nprint(\"\\npredicted values: \")\nprint(label_encoder.inverse_transform(yp))\n\nprint(\"\\nactual values: \")\nprint(label_encoder.inverse_transform(y_encoded))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.support_vectors_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dense Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Create_Model_NN():\n    model = models.Sequential()\n    \n    model.add(layers.Dense(5, input_shape=(50,50,3), activation='relu'))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(13, activation='softmax'))\n    model.summary()\n    \n    model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n    \n    return model\n\ndef train_test_model_NN(model, X_train, y_train, X_test, y_test):\n    history = model.fit(X_train, y_train, epochs=10, \n                    validation_data=(X_test, y_test))\n    \n    plt.plot(history.history['accuracy'], label='accuracy')\n    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.ylim([0.5, 1])\n    plt.legend(loc='lower right')\n\n    test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"model = Create_Model_NN()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"history = train_test_model_NN(model,X_train, y_train, X_test, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ClarifyPrediction_NN(yp):\n    return [np.where(yp[x] == np.amax(yp[x]))[0][0] for x in range(0,len(yp))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"file = listdir(path_test)[0]\nprint(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y_encoded, y = PreprocessSingle(file,path_test,label_encoder)\n\nyp = model.predict(X, batch_size=64)\nyp = ClarifyPrediction_NN(yp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"predicted values (encoded): \")\nprint(yp)\n\nprint(\"\\nactual values (encoded): \")\nprint(y_encoded)\n\nprint(\"\\npredicted values: \")\nprint(label_encoder.inverse_transform(yp))\n\nprint(\"\\nactual values: \")\nprint(label_encoder.inverse_transform(y_encoded)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.get_weights()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convolutional Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Create_Model():\n    model = models.Sequential()\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(50,50, 3)))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(13, activation='softmax'))\n    model.summary()\n    \n    model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n    \n    return model\n\ndef train_test_model(model, X_train, y_train, X_test, y_test):\n    history = model.fit(X_train, y_train, epochs=10, \n                    validation_data=(X_test, y_test))\n    \n    plt.plot(history.history['accuracy'], label='accuracy')\n    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.ylim([0.5, 1])\n    plt.legend(loc='lower right')\n\n    test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n    return history","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model = Create_Model()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"history = train_test_model(model,X_train, y_train, X_test, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ClarifyPrediction(yp):\n    return [np.where(yp[x] == np.amax(yp[x]))[0][0] for x in range(0,len(yp))]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"file = listdir(path_test)[0]\nprint(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y_encoded, y = PreprocessSingle(file,path_test,label_encoder)\n\nyp = model.predict(X, batch_size=64)\nyp = ClarifyPrediction(yp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"predicted values (encoded): \")\nprint(yp)\n\nprint(\"\\nactual values (encoded): \")\nprint(y_encoded)\n\nprint(\"\\npredicted values: \")\nprint(label_encoder.inverse_transform(yp))\n\nprint(\"\\nactual values: \")\nprint(label_encoder.inverse_transform(y_encoded)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.get_weights()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}